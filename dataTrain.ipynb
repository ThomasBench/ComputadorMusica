{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bench\\anaconda3\\envs\\p38\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch import optim, Tensor\n",
    "import math\n",
    "import numpy as np \n",
    "from ast import literal_eval\n",
    "from tqdm.notebook import tqdm\n",
    "from chords_dataset import ChordsDataset\n",
    "from model_helpers import NLP\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = pd.read_csv('./model_data/lyrics_processed.csv')\n",
    "dataset = ChordsDataset(df,NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        num_heads, \n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers, \n",
    "        forward_expansion, \n",
    "        dropout, \n",
    "        max_length, \n",
    "        device \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size).to(device)\n",
    "        self.src_position_embedding = PositionalEncoding( embedding_size,0.1,max_length).to(device)\n",
    "        self.trg_position_embedding = PositionalEncoding( embedding_size,0.1,max_length).to(device)\n",
    "        self.device = device\n",
    "        self.transformer = nn.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads, \n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            forward_expansion,\n",
    "            dropout\n",
    "        ).to(device)\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size).to(device)\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src: Tensor):\n",
    "        shapes = src.shape\n",
    "        mask = torch.zeros((1,shapes[0])).to(device)\n",
    "        return mask\n",
    "    def forward(self, src, trg ):\n",
    "\n",
    "        # print(self.trg_position_embedding.get_device())\n",
    "        embed_trg = self.trg_word_embedding(trg.to(torch.int64).to(device))\n",
    "\n",
    "        \n",
    "        embed_src = self.src_position_embedding(src) \n",
    "        embed_src = self.dropout( embed_src ).to(self.device)\n",
    "        embed_trg = self.dropout( self.trg_position_embedding(embed_trg)).to(self.device)\n",
    "\n",
    "        # print(embed_src.shape, embed_trg.shape)\n",
    "        padding_mask = self.make_src_mask(src).to(self.device)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg.shape[0]).to(self.device)\n",
    "        output = self.transformer(\n",
    "            embed_src,\n",
    "            embed_trg,\n",
    "            src_key_padding_mask = padding_mask,\n",
    "            tgt_mask = trg_mask\n",
    "        )\n",
    "        output = self.fc_out(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/ 3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f75cf3c7d241fda678d80005435b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bench\\Documents\\EPFL\\Master\\MA4\\Computador e musica\\project\\ComputadorMusica\\chords_dataset.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  \"lyrics\": Tensor([self.vectorize(word) for word in  self.lyrics[idx] ]),\n",
      "C:\\Users\\bench\\AppData\\Local\\Temp/ipykernel_30212/2839760000.py:27: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm= 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At i =0 :  7.8258891105651855\n",
      "At i =500 :  0.5942382216453552\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30212/2839760000.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mCHORD_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mtarget_one_hot\u001b[0m\u001b[1;33m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_one_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mchord\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_one_hot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "LYR_VOCAB_SIZE = 100\n",
    "EMB_SIZE = LYR_VOCAB_SIZE\n",
    "CHORD_SIZE = len(dataset.chords_set) \n",
    "num_heads = 5\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "forward_expansion = 1\n",
    "dropout = 0.1 \n",
    "max_length = 7038\n",
    "\n",
    "model = Transformer(EMB_SIZE,LYR_VOCAB_SIZE,CHORD_SIZE,15,num_heads,num_encoder_layers, num_decoder_layers, forward_expansion, dropout, max_length,device ).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 3e-4)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "NB_EPOCHS = 3\n",
    "\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    print(f\"[Epoch {epoch}/ {NB_EPOCHS}]\")\n",
    "    model.train()\n",
    "    for i,elem in tqdm(enumerate(dataset)):\n",
    "        source, target = elem[\"lyrics\"].reshape(-1,1,100).to(device), elem[\"chords\"].reshape(-1,1)\n",
    "        output = model(source, target).reshape(-1,CHORD_SIZE)\n",
    "        optimizer.zero_grad()\n",
    "        target_one_hot=  torch.Tensor([dataset.to_one_hot(int(chord)) for chord in target]).to(device)\n",
    "        loss = criterion(output, target_one_hot)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), max_norm= 1)\n",
    "        optimizer.step()\n",
    "        if i % 500 == 0:\n",
    "            print(f\"At i ={i} : \",float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c08fb7d0681ab80e3191df3cdd3d9ae56e033f792b7fa01572cc6446116ecae"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
